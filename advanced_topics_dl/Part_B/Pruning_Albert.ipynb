{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86916ddb-7b62-4719-aed6-bd5ef186084e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AlbertForSequenceClassification, AlbertTokenizer, Trainer, TrainingArguments\n",
    "from torch.quantization import quantize_dynamic\n",
    "from transformers import Trainer, TrainingArguments, RobertaForSequenceClassification, RobertaTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "import torch\n",
    "from torch.quantization import quantize_dynamic\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "from datasets import load_dataset\n",
    "import torch.nn.utils.prune as prune\n",
    "from transformers import AlbertForSequenceClassification, AlbertTokenizer, Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bb4299f-0ae8-407e-9ea1-59aab18c590d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data\n",
    "export_dir = Path(os.getcwd())\n",
    "data_path = Path(export_dir, \"data\")\n",
    "train_file = Path(data_path, 'train_data_only_text_and_labels.csv')\n",
    "eval_file = Path(data_path, 'eval_data_only_text_and_labels.csv')\n",
    "test_file = Path(data_path, 'test_data_only_text_and_labels.csv')\n",
    "\n",
    "train_df = pd.read_csv(train_file)\n",
    "eval_df = pd.read_csv(eval_file)\n",
    "test_df = pd.read_csv(test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff746448-54ee-4b0f-9b2a-fbb409fa8c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_combined</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mr benson eko bbennisadinetcomuy benson eko ch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>millicent boston helgagermanflintcochraneorg r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jason ling vytekdemisehotmailcom _nextpart_001...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vladimir antalik ohrbzoznamsk would like purch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lennart regebro hyiffbigmailcom wed mar 26 200...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52785</th>\n",
       "      <td>justin shore listuserneopittstateedu 116 pm 04...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52786</th>\n",
       "      <td>charles philip chan optgpesympaticoca aaron ku...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52787</th>\n",
       "      <td>isador kee catimen84swannejp dear d59ebf6a0f14...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52788</th>\n",
       "      <td>global risk management operations weekly opera...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52789</th>\n",
       "      <td>alden merritt gabonysz7onlineno tu peux te fie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>52790 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           text_combined  label\n",
       "0      mr benson eko bbennisadinetcomuy benson eko ch...      1\n",
       "1      millicent boston helgagermanflintcochraneorg r...      1\n",
       "2      jason ling vytekdemisehotmailcom _nextpart_001...      0\n",
       "3      vladimir antalik ohrbzoznamsk would like purch...      0\n",
       "4      lennart regebro hyiffbigmailcom wed mar 26 200...      0\n",
       "...                                                  ...    ...\n",
       "52785  justin shore listuserneopittstateedu 116 pm 04...      0\n",
       "52786  charles philip chan optgpesympaticoca aaron ku...      0\n",
       "52787  isador kee catimen84swannejp dear d59ebf6a0f14...      1\n",
       "52788  global risk management operations weekly opera...      0\n",
       "52789  alden merritt gabonysz7onlineno tu peux te fie...      1\n",
       "\n",
       "[52790 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeabd331-aa62-4204-8907-0f79ad073353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.3.1)\n",
      "Collecting torch\n",
      "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.18.1)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.9.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch)\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.3.1\n",
      "    Uninstalling triton-2.3.1:\n",
      "      Successfully uninstalled triton-2.3.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.3.1\n",
      "    Uninstalling torch-2.3.1:\n",
      "      Successfully uninstalled torch-2.3.1\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.18.1\n",
      "    Uninstalling torchvision-0.18.1:\n",
      "      Successfully uninstalled torchvision-0.18.1\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 torchvision-0.19.0 triton-3.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef3ea1e0-b2e3-4adf-9e82-36843b8b39f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the model from the checkpoint\n",
    "checkpoint_path = \"./results/albert/checkpoint-9900/\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "# Load the model from the checkpoint directory\n",
    "model = AlbertForSequenceClassification.from_pretrained(checkpoint_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd454e6-c8a8-4e6f-87a2-cb5f6313cd4b",
   "metadata": {},
   "source": [
    "### Prune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23790a26-16e0-4f68-980e-3f0ea21e8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to apply global pruning\n",
    "def apply_global_pruning(model, amount=0.3):\n",
    "    # Loop through each module in the model and prune if it is a Linear or Conv layer\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear) or isinstance(module, torch.nn.Conv1d):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            prune.remove(module, 'weight')  # Remove the reparameterization to make it permanent\n",
    "\n",
    "# Apply global pruning to the model (adjust the amount based on your needs)\n",
    "apply_global_pruning(model, amount=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efff081-914f-466a-9cd6-a45a0af98877",
   "metadata": {},
   "source": [
    "### Save pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1332d01-c1b9-4cf0-9759-226c7f822e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"./results/albert/fine_tune/prune/pruned_model.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfb8ee3-bf8e-4b16-bacd-4afbc98b7e12",
   "metadata": {},
   "source": [
    "### Load pruned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fb23d1f-3880-49d0-a10b-23480f180d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2334/2767523246.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"./results/albert/fine_tune/prune/pruned_model.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlbertForSequenceClassification(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the pruned model using metric_fn function\n",
    "# Load the pruned model\n",
    "model = torch.load(\"./results/albert/fine_tune/prune/pruned_model.pth\")\n",
    "# Ensure the model is on the correct device (CPU/GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3430310-13d4-4017-adf8-fe66f3a6a196",
   "metadata": {},
   "source": [
    "### Load original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fd966bb-3e58-4300-a957-3fe1fa01e652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the model from the checkpoint\n",
    "checkpoint_path = \"./results/albert/checkpoint-9900/\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "# Load the model from the checkpoint directory\n",
    "original_model = AlbertForSequenceClassification.from_pretrained(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1389cfa6-7df4-447d-8d19-ca7364c3df85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model non-zero weights: 11685122\n",
      "Pruned model non-zero weights: 9354857\n",
      "Reduction in non-zero weights: 19.94%\n"
     ]
    }
   ],
   "source": [
    "def count_non_zero_weights(model):\n",
    "    non_zero_count = 0\n",
    "    for param in model.parameters():\n",
    "        non_zero_count += torch.sum(param != 0).item()\n",
    "    return non_zero_count\n",
    "\n",
    "# Count non-zero weights in the original model\n",
    "original_non_zero_weights = count_non_zero_weights(original_model)\n",
    "\n",
    "# Count non-zero weights in the pruned model\n",
    "pruned_non_zero_weights = count_non_zero_weights(model)\n",
    "\n",
    "# Print results\n",
    "print(f\"Original model non-zero weights: {original_non_zero_weights}\")\n",
    "print(f\"Pruned model non-zero weights: {pruned_non_zero_weights}\")\n",
    "\n",
    "# Calculate and print the reduction ratio\n",
    "reduction_ratio = (original_non_zero_weights - pruned_non_zero_weights) / original_non_zero_weights\n",
    "print(f\"Reduction in non-zero weights: {reduction_ratio:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a142aff3-22ea-4545-884a-7f4647b02d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def metric_fn(eval_pred):\n",
    "    # Extract predictions and labels\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)  # Convert logits to predicted class\n",
    "    \n",
    "    # Compute basic metrics\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    \n",
    "    # Compute AUC\n",
    "    if len(np.unique(labels)) > 1:  # Check if there is more than one class in labels\n",
    "        auc = roc_auc_score(labels, logits[:, 1])  # Use probability of the positive class\n",
    "    else:\n",
    "        auc = float('nan')  # If only one class is present, AUC is not defined\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'tn': tn,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'tp': tp,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4990f2de-458a-40d6-8d1d-97d6089a8741",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 16498/16498 [00:27<00:00, 589.64 examples/s]\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='330' max='330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [330/330 02:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1724466234445572, 'eval_model_preparation_time': 0.0008, 'eval_accuracy': 0.962783367680931, 'eval_precision': 0.9341545841055271, 'eval_recall': 0.9988343629793682, 'eval_f1': 0.9654123479044615, 'eval_auc': 0.9946599281591365, 'eval_tn': 7315, 'eval_fp': 604, 'eval_fn': 10, 'eval_tp': 8569, 'eval_runtime': 121.7761, 'eval_samples_per_second': 135.478, 'eval_steps_per_second': 2.71}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, RobertaTokenizer\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve\n",
    "\n",
    "# Example test_df DataFrame\n",
    "\n",
    "\n",
    "# Convert test_df to Dataset if it's a DataFrame\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Initialize tokenizer (make sure to use the correct tokenizer for your model)\n",
    "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "\n",
    "# Define a function to tokenize the inputs\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text_combined'], padding='max_length', truncation=True)\n",
    "\n",
    "# Apply the tokenizer to the dataset\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define evaluation arguments\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir='./results/albert/fine_tune/prune',\n",
    "    per_device_eval_batch_size=50,\n",
    "    logging_dir='./results/albert/fine_tune/prune',\n",
    "    evaluation_strategy=\"epoch\"  # Ensure evaluation is done at the end of each epoch\n",
    ")\n",
    "\n",
    "# Define the Trainer with the quantized model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=eval_args,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=metric_fn  # Uncomment if you have a metric function defined\n",
    ")\n",
    "\n",
    "# Evaluate the model on the tokenized test dataset\n",
    "evaluation_results = trainer.evaluate()\n",
    "\n",
    "# Print evaluation results\n",
    "print(evaluation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2ac69-9ef3-4046-800a-d0dac49dddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6d6d3a-be44-4797-849f-c3b8e6adc0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad) * 4 / (1024 ** 2)  # Size in MB\n",
    "get_model_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f1489d-3e71-4dff-9ec4-fb5b47b8b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test= pd.read_csv(Path(data_path,'test_data_only_text_and_labels.csv'))\n",
    "test.head()\n",
    "X_test = test[\"text_combined\"].tolist()\n",
    "y_test = test[\"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b8865-88d0-4e44-a33a-9805dc6038fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216bd3e1-b8ca-41f0-8351-5b7db820bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, precision_recall_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "# Function to evaluate the model on the test set in batches\n",
    "def evaluate_in_batches(model, tokenizer, X_test, y_test, batch_size):\n",
    "    device = torch.device(\"cpu\")  # Change to \"cuda\" if GPU is available and required\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Split the test set into batches\n",
    "    num_batches = len(X_test) // batch_size + (1 if len(X_test) % batch_size != 0 else 0)\n",
    "\n",
    "    all_predictions = []\n",
    "    all_probs = []\n",
    "    all_true_labels = []\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_batches):\n",
    "            # Get the batch data\n",
    "            batch_texts = X_test[i * batch_size: (i + 1) * batch_size]\n",
    "            batch_labels = y_test[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "            # Tokenize the batch\n",
    "            inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "            # Get logits from the model\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits.cpu()\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1).cpu()\n",
    "\n",
    "            # Get the predicted labels\n",
    "            predictions = torch.argmax(probs, dim=1).cpu()\n",
    "\n",
    "            # Store the results\n",
    "            all_predictions.append(predictions.numpy())\n",
    "            all_probs.append(probs[:, 1].numpy())\n",
    "            all_true_labels.append(batch_labels.to_numpy())\n",
    "\n",
    "    # Concatenate all batch results\n",
    "    y_pred = np.concatenate(all_predictions)\n",
    "    y_scores = np.concatenate(all_probs)\n",
    "    y_true = np.concatenate(all_true_labels)\n",
    "    total_time = time.time() - start_time\n",
    "    # Calculate evaluation metrics\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    evaluation_results = {\n",
    "        \"Confusion_Matrix\": cm,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"AUC\": auc,\n",
    "        \"Evaluation_Time\": total_time\n",
    "    }\n",
    "    # Print results\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-Score:\", f1)\n",
    "    print(\"AUC:\", auc)\n",
    "    print(\"eval_time:\", total_time)\n",
    "    # ROC Curve\n",
    "    with open(f'./results/roberta/fine_tune/prune/pruned_model_eval_results_cpu.txt', 'w') as f:\n",
    "        f.write(f\"{evaluation_results}\\n\\n\")\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (area = {auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(y_true, y_scores)\n",
    "    plt.figure()\n",
    "    plt.plot(recall_vals, precision_vals, label='Precision-Recall curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.show()\n",
    "    return cm\n",
    "# Example usage\n",
    "batch_size = 500  # Set the desired batch size\n",
    "cm = evaluate_in_batches(model, tokenizer, X_test, y_test, batch_size)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8683c8fc-a03d-4b6c-bfab-8c24a145988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Define the data matrix\n",
    "data = cm\n",
    "\n",
    "# Define axis labels\n",
    "x_labels = ['Predicted Negative', 'Predicted Positive']\n",
    "y_labels = ['Actual Negative', 'Actual Positive']\n",
    "\n",
    "# Create the heatmap with values inside cells\n",
    "ax = sns.heatmap(data, annot=True, fmt='d', cmap='coolwarm', cbar=True, \n",
    "                 xticklabels=x_labels, yticklabels=y_labels)\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel('Predicted Label')\n",
    "ax.set_ylabel('True Label')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46295d33-56bb-4298-9492-cbdecb401d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
