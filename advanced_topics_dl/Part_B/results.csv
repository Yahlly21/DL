,Model,Loaded from HF,Model name,Model description,Accuracy,Precision,Recall,F1-Score,AUC,TP,FP,FN,TN,Time,Size (Mb),Size (Qty of parameters),path
ALBERT,Ron train,albert-base-v2,Albert - Base - All layers pretrain ,Albert - All layers pretrain checkpoint 9900,0.992302097,0.993000467,0.992190232,0.992595184,0.999639181,8512,60,67,7859,61.4105,44.57520294,11685122,./results/albert/checkpoint-9900/
ALBERT,Yahlly finetune,albert-base-v2,Albert - finetune last 2 layers,Albert - finetune last 2 layers,0.9891502,0.988485694,0.990674904,0.989579088,0.998859158,8499,99,80,7820,,44.57520294,11685122,finetune_ALBERT_epoch_9_acc_0.9901.pt
ALBERT,Ron quant,albert-base-v2,Albert - Quantized,Albert - All layers pretrain checkpoint 9900 - quantized model,0.52721542,0.525218517,0.945564751,0.675324675,0.569606024,8112,7333,467,586,9157.62700414657* Executed on cpu,14.91210938,3909120,./results/albert/fine_tune/quant/quantized_model.pth
ALBERT,Ron prune,albert-base-v2,Albert - Pruned,Albert - All layers pretrain checkpoint 9900 - pruned,0.962783368,0.934154584,0.998834363,0.965412348,0.994659928,8569,604,10,7315,122.04,44.57520294,11685122 (non-zero weights: 9354857 Reduction in non-zero weights: 19.94%),./results/albert/fine_tune/prune/pruned_model.pth
ROBERTA,Ron train,roberta-large,Roberta - Large,Roberta - All layers pretrain 3rd epoch - All layers pretrain checkpoint 9900,0.991574736,0.991040261,0.992773051,0.991905899,0.999306241,8517,77,62,7842,160.1234,1355.597664,355361794,./results/roberta/checkpoint-9900/
ROBERTA,Yahlly finetune,roberta-large,Roberta - finetune last 2 layers,Roberta - finetune last 2 layers,0.994362953,0.995909303,0.993239305,0.994572512,0.999649728,8521,35,58,7884,,1355.597664,355361794,"""finetune_RoBERTa_epoch_9_acc_0.9948.pt"""
ROBERTA,Ron quant,roberta-large,Roberta - Quantized,Roberta quantized model,0.991089829,0.990917559,0.991957105,0.99143706,0.997599721,8510,78,69,7841,8673.0285432338* Executed on cpu,198.7421875,52099072,./results/roberta/fine_tune/quant/quantized_model.pth
ROBERTA,Ron prune,roberta-large,Roberta - Pruned,Roberta - All layers pretrain checkpoint 9900 - pruned,0.990423082,0.986481802,0.995220888,0.990832076,0.999394241,8538,117,41,7802,160.185,1355.597664,"355361794 (non-zero weights 264449629
Reduction in non-zero weights: 25.58%)",./results/roberta/fine_tune/prune/pruned_model.pth
ROBERTA,Ron distill,roberta-base,Roberta - Distilled,Roberta - All layers pretrain checkpoint 3300 - distilled using roberta-base Teacher: Roberta - All layers pretrain checkpoint 9900 Student: roberta-base,0.984846648,0.98050075,0.99055834,0.985503885,0.99871512,8498,169,81,7750,52.4377,475.4912186,124647170,./results/roberta/fine_tune/distilled_model/checkpoint-9900/
DEBERTA,Ron train,microsoft/deberta-large',Deberta - All layers pretrain ,Deberta- All layers pretrain,0.992362711,0.993807688,0.99149085,0.992647917,0.999677533,8506,53,73,7866,,1549.585945,406214658,"""finetune_DeBERTa_epoch_6_acc_0.9951.pt"""
DEBERTA,Yahlly finetune,microsoft/deberta-large',Deberta - finetune last 2 layers,Deberta- finetune last 2 layers,0.992787005,0.99277726,0.993355869,0.99306648,0.999675479,8522,62,57,7857,,1549.585945,406214658,"""finetune_DeBERTa_epoch_6_acc_0.9951.pt"""
ROBERTA,Ron base model,roberta-base,Roberta - All layers pretrain,Roberta base the model training without distilation,0.992362711,0.993807688,0.99149085,0.992647917,0.999677533,8506,53,73,7866,108.8557,475.4912186,124647170,./results/roberta-base_20_epochs/checkpoint-6600/
ALBERT,Ron tune prune tune,albert-base-v2,Albert - Prune + fine tuning,Prune + fine tuning albert,0.990665535,0.988632409,0.993472433,0.991046512,0.999441057,8523,98,56,7821,127.0461,44.57520294,11685122,./results/albert/fine_tune/prune/post_training2/pruned_model.pth
