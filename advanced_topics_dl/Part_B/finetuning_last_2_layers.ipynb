{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea44687-f0d8-43ed-90b7-5ee7d79af174",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install SentencePiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42523689-5320-4005-b2b0-dfdb8784ee9e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6e0cb2-758c-470d-945f-74c61ab83c59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import optuna\n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Huggingface Transformers\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, DebertaTokenizer, DebertaForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments, DistilBertForSequenceClassification, DistilBertTokenizerFast, AlbertTokenizer, AlbertForSequenceClassification\n",
    "\n",
    "# For eval\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a35cfaa-de1f-40bf-bc64-8b764f415f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "export_dir = Path(os.getcwd())\n",
    "data_path = Path(export_dir, \"data\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "models_dir = Path(export_dir, 'models')\n",
    "#os.makedirs(models_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6949dcd-7b4e-40a9-8318-94b445ae8d47",
   "metadata": {},
   "source": [
    "## Load Ron's data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aad3aeb-a60f-4365-9e4f-b227327a5d66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(Path(data_path,'train_data_only_text_and_labels.csv'))\n",
    "test_df = pd.read_csv(Path(data_path,'test_data_only_text_and_labels.csv'))\n",
    "eval_df = pd.read_csv(Path(data_path,'eval_data_only_text_and_labels.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c815cf38-f37e-4a75-a384-22609a199858",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EmailDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer):\n",
    "        self.texts = dataframe['text_combined'].tolist()\n",
    "        self.labels = dataframe['label'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,  ####################### Adjust as needed\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8e938d-6eea-4e39-8aee-d9e8dbf51bfa",
   "metadata": {},
   "source": [
    "# Write here you model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc302ab0-cb3d-4564-a868-7a91f16339f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"RoBERTa\" #phishing-email-detection , DeBERTa, distilbert, ALBERT\n",
    "\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eeebce-fcb9-4cda-9309-ecc0b0ec87ed",
   "metadata": {},
   "source": [
    "# Load Tokenizers and Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f6e33-da94-4e2a-9ff7-5e11005c47b9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if model_name == \"RoBERTa\": \n",
    "    # RoBERTa tokenizer and model\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "    model = RobertaForSequenceClassification.from_pretrained('roberta-large', num_labels=2).to(device)\n",
    "    # Freeze all layers except the last two transformer layers\n",
    "    for param in model.roberta.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.roberta.encoder.layer[-2:].parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "        \n",
    "if model_name == \"DeBERTa\": \n",
    "    # DeBERTa tokenizer and model\n",
    "    tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-large')\n",
    "    model = DebertaForSequenceClassification.from_pretrained('microsoft/deberta-large', num_labels=2).to(device)\n",
    "    # Freeze all layers except the last two transformer layers\n",
    "    for param in model.deberta.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.deberta.encoder.layer[-2:].parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "        \n",
    "if model_name == \"phishing-email-detection\": \n",
    "    # distilbert model and tokenizer\n",
    "    model_name = \"dima806/phishing-email-detection\"\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "    for param in model.distilbert.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.distilbert.transformer.layer[-2:].parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    \n",
    "if model_name == \"distilbert\":\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "    for param in model.distilbert.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Unfreeze only the last two transformer layers\n",
    "    for param in model.distilbert.transformer.layer[-2:].parameters():\n",
    "        param.requires_grad = True\n",
    "    # Ensure the classifier head is trainable\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "        \n",
    "if model_name == \"ALBERT\": \n",
    "    # ALBERT tokenizer and model\n",
    "    tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "    model = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=2).to(device)\n",
    "    \n",
    "    # Freeze all layers except the last two transformer layers\n",
    "    for param in model.albert.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.albert.encoder.albert_layer_groups[-2:].parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a65729-3bc1-400c-8543-ad4105e34d00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = EmailDataset(train_df, tokenizer)\n",
    "val_dataset = EmailDataset(eval_df, tokenizer)\n",
    "test_dataset = EmailDataset(test_df, tokenizer)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847433be-edb3-439f-bd0c-c18ea7a832b7",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a54534-fcf1-4472-839e-189e3db304b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)  # Only optimize the parameters that require gradients\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    i = 0 \n",
    "    for batch in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        i+=1\n",
    "        if i%50==0:\n",
    "            print(i)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and true labels for evaluation\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    # Calculate validation accuracy\n",
    "    val_accuracy = (np.array(val_labels) == np.array(val_preds)).mean()\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    # Save the model after each epoch\n",
    "    model_save_path = Path(models_dir, f'finetune_{model_name}_epoch_{epoch}_acc_{val_accuracy:.4f}.pt')\n",
    "    torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0660e07a-d175-4742-b096-566a2bdf8763",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
